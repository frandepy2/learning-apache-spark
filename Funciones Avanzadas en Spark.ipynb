{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48834af7-78ef-4f01-af23-413d55bb0eb2",
   "metadata": {},
   "source": [
    "# Funciones Avanzadas en Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2adb8a69-ec6c-4342-94d0-3daf14e2444b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/05 05:12:03 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "25/01/05 05:12:03 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "25/01/05 05:12:04 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "25/01/05 05:12:04 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore frandepy@127.0.1.1\n",
      "25/01/05 05:12:05 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
      "25/01/05 05:12:06 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/01/05 05:12:06 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/01/05 05:12:06 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/01/05 05:12:06 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "25/01/05 05:12:07 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/01/05 05:12:07 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/01/05 05:12:07 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/01/05 05:12:07 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "25/01/05 05:12:07 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "25/01/05 05:12:07 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "25/01/05 05:12:07 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "emp = [(1, \"AAA\", \"dept1\", 1000),\n",
    "    (2, \"BBB\", \"dept1\", 1100),\n",
    "    (3, \"CCC\", \"dept1\", 3000),\n",
    "    (4, \"DDD\", \"dept1\", 1500),\n",
    "    (5, \"EEE\", \"dept2\", 8000),\n",
    "    (6, \"FFF\", \"dept2\", 7200),\n",
    "    (7, \"GGG\", \"dept3\", 7100),\n",
    "    (None, None, None, 7500),\n",
    "    (9, \"III\", None, 4500),\n",
    "    (10, None, \"dept5\", 2500)]\n",
    "\n",
    "dept = [(\"dept1\", \"Department - 1\"),\n",
    "        (\"dept2\", \"Department - 2\"),\n",
    "        (\"dept3\", \"Department - 3\"),\n",
    "        (\"dept4\", \"Department - 4\")\n",
    "       ]\n",
    "\n",
    "df = spark.createDataFrame(emp, [\"id\", \"name\", \"dept\", \"salary\"])\n",
    "deptdf = spark.createDataFrame(dept, [\"id\", \"name\"]) \n",
    "\n",
    "# Create Temp Tables\n",
    "df.createOrReplaceTempView(\"empdf\")\n",
    "deptdf.createOrReplaceTempView(\"deptdf\")\n",
    "\n",
    "# Save as HIVE tables.\n",
    "df.write.saveAsTable(\"hive_empdf\", mode = \"overwrite\")\n",
    "deptdf.write.saveAsTable(\"hive_deptdf\", mode = \"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f37cf6-1854-475c-bd5d-9c274466d19b",
   "metadata": {},
   "source": [
    "# Broadcast JOIN: \n",
    "El objetivo de este tipo de union es mejorar el rendimiento de los dataframes, es una optimizacion para realizar un join de manera mas eficiente, cuando una de las tablas es mas pequenha que las otras.\n",
    "\n",
    "## Funcionamiento:\n",
    "Un broadcast join difunde (broadcast) la tabla mas pequena a todos los nodos del cluster, Esto significa que cada nodo recibe una copia de la tabla mas pequena lo que permite realizar el join localmente en cada nodo sin necesidad de cambiar los datos entre ellos.\n",
    "\n",
    "## Ventajas\n",
    "- Reduccion del trafico de red.\n",
    "- Mayor velocidad\n",
    "- Optimizacion Automatica.\n",
    "\n",
    "## Limitaciones:\n",
    "- La tabla pequena debe caber en la memoria de cada nodo, ya que este se replica completamente, si es muy grande puede causar errores de memoria.\n",
    "- Carga en la memoria: Requiere suficiente espacio de memoria de cada nodo para almacenar la tabla pequena.\n",
    "\n",
    "## Casos de uso\n",
    "- Unir una tabla pequena (ej: Lista de parametros) con una tabla grande.\n",
    "\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f5a518-6941-4ffa-b799-d86303e24a22",
   "metadata": {},
   "source": [
    "## Codigo para determinar el tamano predeterminado de la tabla broadcast en Apache Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6139c13b-4ffe-4ae1-ad2c-e684894817c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default size of broadcast table is 10.0 MB.\n"
     ]
    }
   ],
   "source": [
    "size_str = spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")\n",
    "size = int(''.join([char for char in size_str if char.isdigit()])) / (1024 * 1024)\n",
    "print(\"Default size of broadcast table is {0} MB.\".format(size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0520a1d2-a754-4a46-a181-e004ec7859b5",
   "metadata": {},
   "source": [
    "## Se puede cambiar el umbral o tamano por defecto con el siguiente comando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e41c9e8-7955-48c7-b9d0-e0b075ea296d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 50 * 1024 * 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d52583c-3ae1-42cb-ba2e-9c032781ef8c",
   "metadata": {},
   "source": [
    "## Ejemplo\n",
    "Supongamos que queremos unir 2 datafremes, uno `small_df` que puede caber en la memoria y es mas pequeno que el umbral especificado y `big_df` que es un dataframe que debe unirse con el pequeno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "694248c1-d7f7-43fe-bbd6-760ece0999bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------+-------+\n",
      "| id|      date|amount|country|\n",
      "+---+----------+------+-------+\n",
      "|  1|2025-01-01| 100.0|    USA|\n",
      "|  2|2025-01-02| 200.0|    CAN|\n",
      "|  3|2025-01-03| 300.0|    MEX|\n",
      "|  4|2025-01-04| 400.0|    USA|\n",
      "|  5|2025-01-05| 500.0|    CAN|\n",
      "+---+----------+------+-------+\n",
      "\n",
      "+----+-------------+\n",
      "|code|         name|\n",
      "+----+-------------+\n",
      "| USA|United States|\n",
      "| CAN|       Canada|\n",
      "| MEX|       Mexico|\n",
      "+----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_data = [\n",
    "    (1, \"2025-01-01\", 100.0, \"USA\"),\n",
    "    (2, \"2025-01-02\", 200.0, \"CAN\"),\n",
    "    (3, \"2025-01-03\", 300.0, \"MEX\"),\n",
    "    (4, \"2025-01-04\", 400.0, \"USA\"),\n",
    "    (5, \"2025-01-05\", 500.0, \"CAN\"),\n",
    "]\n",
    "\n",
    "transactions_schema = [\"id\", \"date\", \"amount\", \"country\"]\n",
    "\n",
    "transactions_df = spark.createDataFrame(transactions_data, schema=transactions_schema)\n",
    "\n",
    "# Crear una tabla pequeña de países\n",
    "countries_data = [\n",
    "    (\"USA\", \"United States\"),\n",
    "    (\"CAN\", \"Canada\"),\n",
    "    (\"MEX\", \"Mexico\"),\n",
    "]\n",
    "\n",
    "countries_schema = [\"code\", \"name\"]\n",
    "countries_df = spark.createDataFrame(countries_data, schema=countries_schema)\n",
    "\n",
    "transactions_df.show()\n",
    "countries_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3970b099-f8a6-4247-9ef8-053b416fd124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regular JOIN\n",
      "+---+----------+------+-------+----+-------------+\n",
      "| id|      date|amount|country|code|         name|\n",
      "+---+----------+------+-------+----+-------------+\n",
      "|  2|2025-01-02| 200.0|    CAN| CAN|       Canada|\n",
      "|  5|2025-01-05| 500.0|    CAN| CAN|       Canada|\n",
      "|  3|2025-01-03| 300.0|    MEX| MEX|       Mexico|\n",
      "|  1|2025-01-01| 100.0|    USA| USA|United States|\n",
      "|  4|2025-01-04| 400.0|    USA| USA|United States|\n",
      "+---+----------+------+-------+----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# realizamos un join normal de ambos\n",
    "regular_join_df = transactions_df.join(countries_df, transactions_df.country == countries_df.code)\n",
    "print(\"regular JOIN\")\n",
    "regular_join_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e08da361-4055-40bb-9cf4-34587f2fa004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Broadcast JOIN\n",
      "+---+----------+------+-------+----+-------------+\n",
      "| id|      date|amount|country|code|         name|\n",
      "+---+----------+------+-------+----+-------------+\n",
      "|  1|2025-01-01| 100.0|    USA| USA|United States|\n",
      "|  2|2025-01-02| 200.0|    CAN| CAN|       Canada|\n",
      "|  3|2025-01-03| 300.0|    MEX| MEX|       Mexico|\n",
      "|  4|2025-01-04| 400.0|    USA| USA|United States|\n",
      "|  5|2025-01-05| 500.0|    CAN| CAN|       Canada|\n",
      "+---+----------+------+-------+----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Uitlizando Broadcast\n",
    "\n",
    "broadcast_join_df = transactions_df.join(broadcast(countries_df),transactions_df.country == countries_df.code)\n",
    "print(\"Broadcast JOIN\")\n",
    "broadcast_join_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2f727c-5f30-476c-a86b-b7caf99e3dc0",
   "metadata": {},
   "source": [
    "# Almacenamiento en Cache en Spark\n",
    "\n",
    "En spark, el almacenamiento en cache permite almacenar en memoria (o disco) los resultados intermedios de las transformaciones en un dataframe o RDD, evitando que se recalculen.\n",
    "\n",
    "## Proposito del Cache en Spark\n",
    "- Eficiencia computacional\n",
    "    - Spark utiliza `lazy evaluation`, lo que significa que las transformaciones no se ejecutan hasta que se realiza una accion.\n",
    "    - Sin Cache, cada vez que se realiza una accion, Spark recalcula todos los pasos desde el principio.\n",
    "- Reutilizacion de datos\n",
    "    - Si necesitas reutilizar el resultado de una transformacion intermedia en multiples operaciones, cache evita la recalculacion.\n",
    "\n",
    "## Operaciones de cache con Spark\n",
    "- `cache()` : Indica que los datos deben almacenarse en memoria por defecto, el nivel de almacenamiento predeterminado: `MEMORY_AND_DISK` (almacena en memoria, si no hay suficiente memoria escribe en disco).\n",
    "- `persist()` : Mas flexible que `cache()` ya que permite especificar niveles de almacenamiento.\n",
    "\n",
    "```python\n",
    "df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "```\n",
    "\n",
    "Podemos usar la funcion de Cache/persist para mantener el marco de datos en la memoria, Puede mejorar significativamente el rendimiento de la aplicacion Spark, si alamacenamos en Cache los datos que necesitamos usar con mucha frecuencia en nuestra aplicacion.\n",
    "\n",
    "## Niveles de almacenamiento\n",
    "\n",
    "| NIvel de Almacenamiento | Descripcion |\n",
    "|--------------|--------------|\n",
    "| `MEMORY_ONLY` | Almacena en memoria. Si no cabe, se recalcula cuando es necesario |\n",
    "| `MEMORY_AND_DISK` | Almacena en memoria, si no cabe se reescribe en disco |\n",
    "| `MEMORY_ONLY_SER` | Lo mismo que `MEMORY_ONLY`, pero serializado (Reduce el uso de memoria) |\n",
    "| `MEMORY_AND_DISK_SER` | Igual que `MEMORY_AND_DISK` pero serializado |\n",
    "| `DISK_ONLY` | Solamente en disco |\n",
    "| `OFF_HEAP` |  Usa memoria off-heap (fuera del heap de JAVA), Nesecita configuracion extra |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba0e5c33-67f6-4e6d-a322-c03222e05d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Used : True\n",
      "Disk Used : True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/05 05:39:18 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    }
   ],
   "source": [
    "# Cachea el dataframe\n",
    "df.cache()\n",
    "\n",
    "# Fuerza el almacenamiento en cache\n",
    "df.count()\n",
    "\n",
    "print(\"Memory Used : {0}\".format(df.storageLevel.useMemory))\n",
    "print(\"Disk Used : {0}\".format(df.storageLevel.useDisk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "243f3dd3-d682-4748-9b38-65d68c99b8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# En caso que no queramos mas mantener en memoria podemos eliminar el cache. Es una buena practica para cuando ya no requeramos de los datos.\n",
    "df.unpersist()\n",
    "\n",
    "sqlContext.clearCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831e4111-6431-4ac9-9b65-93e9abc6e23c",
   "metadata": {},
   "source": [
    "# Expreciones con SQL\n",
    "Se utilizan para ejecutar expreciones SQL directamente dentro de la API de DataFrame, estas funciones son herramientas poderosas cuando queremos aplicas logica SQL sin usar un contexto SQL.\n",
    "`exp` permite evaluar una expresion SQL en el contexto de un Dataframe, se usa para generar columnas o realizar transformaciones en las existentes.\n",
    "\n",
    "la funcion `selectExpr` permite seleccionar columnas de un dataframe utilizando expresiones SQL. es una version mas poderosa de select ya que admite expresiones SQL complejas que permite realizar calculos en Linea.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "800c8776-df5f-49cb-872e-b411dfc1326b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+------+------------+\n",
      "|  id|name| dept|salary|salary_level|\n",
      "+----+----+-----+------+------------+\n",
      "|   1| AAA|dept1|  1000|  low_salary|\n",
      "|   2| BBB|dept1|  1100|  low_salary|\n",
      "|   3| CCC|dept1|  3000|  mid_salary|\n",
      "|   4| DDD|dept1|  1500|  low_salary|\n",
      "|   5| EEE|dept2|  8000| high_salary|\n",
      "|   6| FFF|dept2|  7200| high_salary|\n",
      "|   7| GGG|dept3|  7100| high_salary|\n",
      "|NULL|NULL| NULL|  7500| high_salary|\n",
      "|   9| III| NULL|  4500|  mid_salary|\n",
      "|  10|NULL|dept5|  2500|  mid_salary|\n",
      "+----+----+-----+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "cond = \"\"\"case when salary > 5000 then 'high_salary'\n",
    "            else case when salary > 2000 then 'mid_salary'\n",
    "                else case when salary > 0 then 'low_salary'\n",
    "                    else 'invalid_salary'\n",
    "                        end\n",
    "                    end\n",
    "                end as salary_level\"\"\"\n",
    "\n",
    "newdf = df.withColumn(\"salary_level\", expr(cond))\n",
    "\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bef36f80-4fbf-4469-a8ab-33643f3d9335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+------+------------+\n",
      "|  id|name| dept|salary|salary_level|\n",
      "+----+----+-----+------+------------+\n",
      "|   1| AAA|dept1|  1000|  low_salary|\n",
      "|   2| BBB|dept1|  1100|  low_salary|\n",
      "|   3| CCC|dept1|  3000|  mid_salary|\n",
      "|   4| DDD|dept1|  1500|  low_salary|\n",
      "|   5| EEE|dept2|  8000| high_salary|\n",
      "|   6| FFF|dept2|  7200| high_salary|\n",
      "|   7| GGG|dept3|  7100| high_salary|\n",
      "|NULL|NULL| NULL|  7500| high_salary|\n",
      "|   9| III| NULL|  4500|  mid_salary|\n",
      "|  10|NULL|dept5|  2500|  mid_salary|\n",
      "+----+----+-----+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf = df.selectExpr(\"*\",cond)\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49c11f58-dda6-4121-a5be-3d7afa913c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+\n",
      "|salary|salary_level|\n",
      "+------+------------+\n",
      "|  1000|  low_salary|\n",
      "|  1100|  low_salary|\n",
      "|  3000|  mid_salary|\n",
      "|  1500|  low_salary|\n",
      "|  8000| high_salary|\n",
      "|  7200| high_salary|\n",
      "|  7100| high_salary|\n",
      "|  7500| high_salary|\n",
      "|  4500|  mid_salary|\n",
      "|  2500|  mid_salary|\n",
      "+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf = df.selectExpr(\"salary\",cond)\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e479788-441d-4050-b96b-7f665ddeea41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones Definidas por el usuario\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
